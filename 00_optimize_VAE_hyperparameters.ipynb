{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "import kerastuner\n",
    "from kerastuner.tuners import Hyperband, BayesianOptimization\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/TwinsUK.xls'\n",
    "\n",
    "tw_train_data = pd.read_excel(data_path, sheet_name='Training Set')\n",
    "tw_test_data = pd.read_excel(data_path, sheet_name='Testing Set')\n",
    "\n",
    "twins_data = pd.concat([tw_train_data, tw_test_data], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data & model configuration\n",
    "batch_size = 32\n",
    "no_epochs = 1000\n",
    "latent_dim = 18\n",
    "\n",
    "original_dim = tw_train_data.shape[1]\n",
    "input_shape = (original_dim,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# recommended to do this here: https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "    def on_train_end(*args, **kwargs):\n",
    "        IPython.display.clear_output(wait = True)\n",
    "        \n",
    "        \n",
    "def model_builder(hp):\n",
    "    # # =================\n",
    "    # # Encoder\n",
    "    # # =================\n",
    "\n",
    "    # Definition\n",
    "    i       = Input(shape=input_shape, name='encoder_input')\n",
    "    \n",
    "    x       = Dense(hp.Int('encoder_units',\n",
    "                           min_value=30,\n",
    "                           max_value=220,\n",
    "                           step=10))(i)\n",
    "    x       = LeakyReLU()(x)\n",
    "    \n",
    "    mu      = Dense(latent_dim, name='latent_mu')(x)\n",
    "    sigma   = Dense(latent_dim, name='latent_sigma')(x)\n",
    "\n",
    "    # Define sampling with reparameterization trick\n",
    "    def sample_z(args):\n",
    "        mu, sigma = args\n",
    "        batch     = K.shape(mu)[0]\n",
    "        dim       = K.int_shape(mu)[1]\n",
    "        eps       = K.random_normal(shape=(batch, dim))\n",
    "        return mu + K.exp(sigma / 2) * eps\n",
    "\n",
    "    # Use reparameterization trick to ....??\n",
    "    z       = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([mu, sigma])\n",
    "\n",
    "    # Instantiate encoder\n",
    "    encoder = Model(i, [mu, sigma, z], name='encoder')\n",
    "    \n",
    "    # =================\n",
    "    # Decoder\n",
    "    # =================\n",
    "\n",
    "    # Definition\n",
    "    d_i   = Input(shape=(latent_dim, ), name='decoder_input')\n",
    "    \n",
    "    x     = Dense(hp.Int('decoder_units',\n",
    "                           min_value=20,\n",
    "                           max_value=220,\n",
    "                           step=10))(d_i)\n",
    "    x     = LeakyReLU()(x)\n",
    "        \n",
    "    o     = Dense(original_dim)(x)\n",
    "\n",
    "    # Instantiate decoder\n",
    "    decoder = Model(d_i, o, name='decoder')\n",
    "    \n",
    "    # =================\n",
    "    # VAE as a whole\n",
    "    # =================\n",
    "\n",
    "    # Define loss\n",
    "    def kl_reconstruction_loss(true, pred):\n",
    "      # Reconstruction loss\n",
    "        reconstruction_loss = mse(true, pred)\n",
    "        reconstruction_loss *= original_dim\n",
    "\n",
    "        # KL divergence loss\n",
    "        kl_loss = 1 + sigma - K.square(mu) - K.exp(sigma)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        \n",
    "        # weight KL divergence loss here\n",
    "        kl_loss *= hp.Float(\n",
    "        'kl_beta',\n",
    "        min_value=1e-3,\n",
    "        max_value=1e1,\n",
    "        sampling='LOG',\n",
    "        default=1e-2\n",
    "        )\n",
    "\n",
    "        return K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "    # Instantiate VAE\n",
    "    vae_outputs = decoder(encoder(i)[2])\n",
    "    vae         = Model(i, vae_outputs, name='vae')\n",
    "\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = Adam(hp.Float(\n",
    "        'learning_rate',\n",
    "        min_value=1e-4,\n",
    "        max_value=1e-2,\n",
    "        sampling='LOG',\n",
    "        default=1e-3\n",
    "    ))\n",
    "\n",
    "    # Compile VAE\n",
    "    vae.compile(optimizer=optimizer, loss=kl_reconstruction_loss, metrics = ['mse'], experimental_run_tf_function=False)\n",
    "    \n",
    "    return vae\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set tuner parameters\n",
    "tuner = Hyperband(\n",
    "    model_builder,\n",
    "    objective='mse',\n",
    "    factor=2,\n",
    "    max_epochs=200,\n",
    "    directory='hyperband_optimization',\n",
    "    project_name='mtvae')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run Tuner\n",
    "# Runtime on MacBook Pro 2017: 01h 15m\n",
    "tuner.search(\n",
    "    tw_train_data, tw_train_data, \n",
    "    validation_data = (tw_test_data, tw_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out best tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuner.results_summary(num_trials = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.get_best_models()[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print(best_hps.get('encoder_units'))\n",
    "print(best_hps.get('decoder_units'))\n",
    "print(best_hps.get('learning_rate'))\n",
    "print(best_hps.get('kl_beta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
